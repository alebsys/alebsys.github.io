<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Linux on Performance Matters</title>
    <link>https://alebedev.tech/tags/linux/</link>
    <description>Recent content in Linux on Performance Matters</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Mon, 14 Oct 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://alebedev.tech/tags/linux/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Kafka и медленная запись</title>
      <link>https://alebedev.tech/posts/kafka-ssl/</link>
      <pubDate>Mon, 14 Oct 2024 00:00:00 +0000</pubDate>
      <guid>https://alebedev.tech/posts/kafka-ssl/</guid>
      <description>Разработчики провели нагрузочные тесты и выявили значительные задержки при записи данных в Kafka, что приводило к накоплению лага.
Я подключился к анализу причин низкой производительности.
Сильно упрощенный флоу:
приложение пишет в Kafka (кластер на виртуальных машинах); Kafka работает в режиме acks=all и не поспевает за потоком событий; на приложении копится очередь на запись. Прежде чем детально разбираться с Kafka, я бегло проверил сетевые ресурсы и состояние машин. Аномалий выявлено не было.</description>
    </item>
    <item>
      <title>&#34;bash: curl: command not found&#34;</title>
      <link>https://alebedev.tech/posts/bash-curl-not-found/</link>
      <pubDate>Tue, 27 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://alebedev.tech/posts/bash-curl-not-found/</guid>
      <description>Или как траблшутить контейнеры когда нечем.</description>
    </item>
    <item>
      <title>TCP Congestion Control in Action</title>
      <link>https://alebedev.tech/posts/cwnd-retransmit/</link>
      <pubDate>Thu, 08 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://alebedev.tech/posts/cwnd-retransmit/</guid>
      <description>Протестировал работу TCP Congestion Control в Linux в различных по качеству окружениях, поделюсь результатами.</description>
    </item>
    <item>
      <title>Memory Soft limits</title>
      <link>https://alebedev.tech/posts/memory-soft-limit/</link>
      <pubDate>Sun, 16 Jun 2024 00:00:00 +0000</pubDate>
      <guid>https://alebedev.tech/posts/memory-soft-limit/</guid>
      <description>Изучая возможности контрольных групп в linux наткнулся на параметр memory.soft_limit_in_bytes:
When the system detects memory contention or low memory, control groups are pushed back to their soft limits. If the soft limit of each control group is very high, they are pushed back as much as possible to make sure that one control group does not starve the others of memory.
Аналог в cgroup2 - memory.low
Звучит интересно! Сделаем несколько тестов для понимания как это все работает.</description>
    </item>
    <item>
      <title>Где мой трафик?</title>
      <link>https://alebedev.tech/posts/metallb-calico-ebpf/</link>
      <pubDate>Thu, 23 May 2024 00:00:00 +0000</pubDate>
      <guid>https://alebedev.tech/posts/metallb-calico-ebpf/</guid>
      <description>Наблюдаемость и простота отладки одни из важнейших свойств системы, которые следует учитывать уже на этапе выбора технологий. В противном случае может быть больно и дорого.
Следующий этап - разобраться как технология устроена и желательно сделать это еще до развертывания в боевых окружениях. Но бывает конечно по всякому.
Сегодня рассмотрим сеть в Kubernetes и выявим несоответствия в учете трафика, которые могут вводить в заблуждение.
Дано:
Kubernetes кластер; MetalLB L2 как loadBalancer; Nginx Ingress Controller; Calico на ebpf.</description>
    </item>
    <item>
      <title>Long way to the application, №2.</title>
      <link>https://alebedev.tech/posts/long-way-to-the-app-2/</link>
      <pubDate>Tue, 14 May 2024 00:00:00 +0000</pubDate>
      <guid>https://alebedev.tech/posts/long-way-to-the-app-2/</guid>
      <description>Привет!
Продолжаем изучать маршрут сетевого пакета по подсистемам ядра linux.
В первой части мы разобрали отрезок от сетевой карты гипервизора до txqueue - очередь перед виртуальной машиной в которой и крутится наш сервис.
Теперь разберем следующий отрезок - от сетевой карты ВМ и до самого приложения.
RX queue Процесс обработки трафика виртуальной машиной идентичен с гипервизоров - softirqd демоны разгребают RX очереди. Ничего нового.
Различия начинаются на уровнях сетевого стека, точнее перед ним.</description>
    </item>
    <item>
      <title>Long way to the application, №1.</title>
      <link>https://alebedev.tech/posts/long-way-to-the-app-1/</link>
      <pubDate>Sun, 21 Apr 2024 00:00:00 +0000</pubDate>
      <guid>https://alebedev.tech/posts/long-way-to-the-app-1/</guid>
      <description>Прежде чем сетевой пакет из сервиса А попадет в сервис Б, ему следует миновать множество систем по середине&amp;hellip;</description>
    </item>
    <item>
      <title>Tooling. Cache Hit Ratio</title>
      <link>https://alebedev.tech/posts/cachestat-ebpf-exporter/</link>
      <pubDate>Tue, 16 Apr 2024 00:00:00 +0000</pubDate>
      <guid>https://alebedev.tech/posts/cachestat-ebpf-exporter/</guid>
      <description>Об утилите cachestat я узнал из статьи Brendan Gregg - Analyzing a High Rate of Paging.
В ней расследуются причины аномально долгого процессинга файлов размером в 100Gbytes, тогда как файлы в 40Gbytes обрабатывались на несколько порядков быстрее.
В том кейсе точно поставить диагноз - большой объект не помещался в кеш, что приводило к trashing&amp;rsquo;у страниц памяти, автору помог инструмент cachestat.
Если в кратце, то утилита подсчитывает соотношение попаданий в кеш файловой системы относительно общего числа операций на чтение - cache hit ratio:</description>
    </item>
    <item>
      <title>Tooling. Latency I/O операций</title>
      <link>https://alebedev.tech/posts/xfs-ebpf-exporter/</link>
      <pubDate>Fri, 05 Apr 2024 00:00:00 +0000</pubDate>
      <guid>https://alebedev.tech/posts/xfs-ebpf-exporter/</guid>
      <description>Проблематика Чтобы оценить производительность ввода/вывода обычно принято обращаться к метрикам дисковой подсистемы, благо они богато представлены “стандартными” инструментами linux - sar, iostat, atop, etc.
Проблема данного подхода, в упрощении общей картины, он игнорирует такой важный компонент в работе I/O подсистемы как файловая система.
Например:
приложение может использовать асинхронную модель записи и flush на диск происходит порциями и когда-то потом; приложение хочет прочитать 1 байт данных, но считать с диска возможно только блоком в 4096 байт; используется read-ahead при последовательном чтении; чтение происходит из кеша файловой системы - запросов на чтение много, а диск почти не загружен; и так далее.</description>
    </item>
    <item>
      <title>Haproxy memory leak</title>
      <link>https://alebedev.tech/posts/haproxy-mem-leak/</link>
      <pubDate>Sun, 31 Mar 2024 00:00:00 +0000</pubDate>
      <guid>https://alebedev.tech/posts/haproxy-mem-leak/</guid>
      <description>На днях диагностировал утечку памяти в кластере haproxy, в процессе исследования допустил несколько ошибок, что привело к более долгой локализации проблемы.
На примере данного кейса хочу провести небольшое ретро.
Моя основная деятельность связана с поиском и устранением проблем, негативно влияющих на производительность системы в целом, будь то runtime приложения, сеть, слой виртуализации, база данных или что-угодно еще.
В этот раз по OOM прибило несколько экземпляров haproxy - согласитесь событие нестандартное. Требовалось понять причины и предложить решение.</description>
    </item>
    <item>
      <title>Tooling. TCP ретрансмиты и их направления</title>
      <link>https://alebedev.tech/posts/tcp-retransmit-ebpf-exporter/</link>
      <pubDate>Tue, 19 Mar 2024 00:00:00 +0000</pubDate>
      <guid>https://alebedev.tech/posts/tcp-retransmit-ebpf-exporter/</guid>
      <description>Объем TCP ретрансмитов пожалуй один из наиболее говорящих симптомов деградации системы.
Причины возникновения могут быть различны, самые частые в моей практике:
переполнения очередей, на уровнях сетевого оборудования, NIC, операционной системы, сокетов, etc; out of order пакеты, связанные например с неоднородными маршрутами в рамках одного сетевого потока. В момент проблемы хочется быстро ее обнаружить и определить источник.
Стандартные утилиты На помощь приходят встроенные в linux утилиты:
netstat -s | grep -i retr 3951614 segments retransmitted 1434 times recovered from packet loss due to fast retransmit Detected reordering 149 times using reno fast retransmit TCPLostRetransmit: 2931590 981 timeouts after reno fast retransmit 3538 fast retransmits 4126 retransmits in slow start TCPSynRetrans: 3918500 Те же метрики мы можем собрать и node_exporter’ом: Проблема в слишком “общей” картине таких метрик.</description>
    </item>
  </channel>
</rss>
